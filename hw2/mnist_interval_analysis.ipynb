{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe70a96",
   "metadata": {},
   "source": [
    "## Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b99c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6597.46s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/nalin/miniconda3/lib/python3.13/site-packages (3.10.6)\n",
      "Requirement already satisfied: tensorboardX in /home/nalin/miniconda3/lib/python3.13/site-packages (2.6.4)\n",
      "Requirement already satisfied: torchvision in /home/nalin/miniconda3/lib/python3.13/site-packages (0.23.0)\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nalin/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/nalin/miniconda3/lib/python3.13/site-packages (from tensorboardX) (6.32.1)\n",
      "Requirement already satisfied: torch==2.8.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nalin/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nalin/miniconda3/lib/python3.13/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib tensorboardX torchvision torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bbda12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize()\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (5): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Simple NN. You can change this if you want. If you change it, mention the architectural details in your report.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 28*28))\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1) # added softmax for probabilities\n",
    "        return x\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# Add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "prev_model = nn.Sequential(Normalize(), Net()).to(device)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Normalize(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28,200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.Softmax(dim=-1),\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8cbece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Normalize-1               [-1, 28, 28]               0\n",
      "           Flatten-2                  [-1, 784]               0\n",
      "            Linear-3                  [-1, 200]         157,000\n",
      "              ReLU-4                  [-1, 200]               0\n",
      "            Linear-5                   [-1, 10]           2,010\n",
      "           Softmax-6                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 0.62\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Normalize-1               [-1, 28, 28]               0\n",
      "            Linear-2                  [-1, 200]         157,000\n",
      "            Linear-3                   [-1, 10]           2,010\n",
      "               Net-4                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 0.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (28, 28))\n",
    "summary(prev_model, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a6282f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.3f}')\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f'Accuracy on images: {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72dd8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.002\n",
      "Epoch 2/10, Loss: 1.700\n",
      "Epoch 3/10, Loss: 1.649\n",
      "Epoch 4/10, Loss: 1.602\n",
      "Epoch 5/10, Loss: 1.584\n",
      "Epoch 6/10, Loss: 1.574\n",
      "Epoch 7/10, Loss: 1.567\n",
      "Epoch 8/10, Loss: 1.562\n",
      "Epoch 9/10, Loss: 1.558\n",
      "Epoch 10/10, Loss: 1.554\n",
      "Accuracy on images: 92.53\n"
     ]
    }
   ],
   "source": [
    "train_model(model, 10)\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c23f92",
   "metadata": {},
   "source": [
    "### Write the interval analysis for the simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Write the interval analysis for the simple model\n",
    "## you can use https://github.com/Zinoex/bound_propagation\n",
    "\n",
    "### Custom Bounding framework\n",
    "# I wrote a simple factory similar to one in bound_propogation to get bounds for any simple nn model\n",
    "\n",
    "class Bounds:\n",
    "    def __init__(self, lower, upper):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "class BoundedModule(nn.Module):\n",
    "    def __init__(self, model_module):\n",
    "        super().__init__()\n",
    "        self.model_module = model_module\n",
    "    def forward(self, x):\n",
    "        return self.model_module(x)\n",
    "    def interval_forward(self, bounds):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BoundedLinear(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        weights = self.model_module.weight\n",
    "        bias = self.model_module.bias\n",
    "\n",
    "        weight_pos = torch.clamp(weights, min=0) # tensor of positive wieghts | 0\n",
    "        weight_neg = torch.clamp(weights, max=0) # tensor of negative weights | 0\n",
    "        \n",
    "        # Interval arithmetic for y = Wx + b\n",
    "        lower = F.linear(bounds.lower, weight_pos, bias) + F.linear(bounds.upper, weight_neg, None)\n",
    "        upper = F.linear(bounds.upper, weight_pos, bias) + F.linear(bounds.lower, weight_neg, None)\n",
    "        \n",
    "        return Bounds(lower, upper)\n",
    "\n",
    "class BoundedReLU(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        lower = torch.clamp(bounds.lower, min=0)\n",
    "        upper = torch.clamp(bounds.upper, min=0)\n",
    "\n",
    "        return Bounds(lower, upper)\n",
    "\n",
    "class BoundedSoftmax(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        dim = self.model_module.dim\n",
    "        exp_lower = torch.exp(bounds.lower)\n",
    "        exp_upper = torch.exp(bounds.upper)\n",
    "        sum_exp_lower = exp_lower.sum(dim=dim, keepdim=True)\n",
    "        sum_exp_upper = exp_upper.sum(dim=dim, keepdim=True)\n",
    "        lower = exp_lower / sum_exp_upper\n",
    "        upper = exp_upper / sum_exp_lower\n",
    "        return Bounds(lower, upper)\n",
    "\n",
    "class BoundedFlatten(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        batch_size = bounds.lower.size(0)\n",
    "        lower = bounds.lower.view(batch_size, -1)\n",
    "        upper = bounds.upper.view(batch_size, -1)\n",
    "        return Bounds(lower, upper)\n",
    "\n",
    "class BoundedSequential(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        self.bounded_modules = [get_bounded_module(child) for child in self.model_module.children()]\n",
    "        \n",
    "        for bounded_module in self.bounded_modules:\n",
    "            bounds = bounded_module.interval_forward(bounds)\n",
    "        \n",
    "        return bounds\n",
    "\n",
    "class BoundedMonotonic(BoundedModule):\n",
    "    def interval_forward(self, bounds):\n",
    "        # assuming that the generic module implements some sort of monotonic function\n",
    "        lower = self.model_module.forward(bounds.lower)\n",
    "        upper = self.model_module.forward(bounds.upper)\n",
    "        return Bounds(lower, upper)\n",
    "\n",
    "\n",
    "BoundedModuleRegistery = {\n",
    "    nn.Linear : BoundedLinear,\n",
    "    nn.ReLU : BoundedReLU,\n",
    "    nn.Sequential : BoundedSequential,\n",
    "    nn.Flatten : BoundedFlatten,\n",
    "    nn.Softmax : BoundedSoftmax,\n",
    "}\n",
    "\n",
    "def get_bounded_module(module: nn.Module) -> BoundedModule:\n",
    "    module_type = type(module)\n",
    "    if module_type not in BoundedModuleRegistery:\n",
    "        # print(f\"ALERT: Using BoundedMonotonic for module {module} of type {module_type}. Only ignore this alert if this module is monotonic\")\n",
    "        return BoundedMonotonic(module)\n",
    "    return BoundedModuleRegistery[module_type](module)\n",
    "\n",
    "def get_input_bounds(x, eps):\n",
    "    lower = torch.clamp(x-eps, min=0, max=1)\n",
    "    upper = torch.clamp(x+eps, min=0, max=1)\n",
    "    return Bounds(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ade2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_model = get_bounded_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0b67ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eps: 0.01\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 546 (5.46%)\n"
     ]
    }
   ],
   "source": [
    "def test_model_robustness(bounded_model, eps):\n",
    "    bounded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        robust = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = bounded_model.forward(images)\n",
    "            perterbed_images_bounds = get_input_bounds(images, eps)\n",
    "            output_bounds = bounded_model.interval_forward(perterbed_images_bounds)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # check to see if any lower bound is larger than all the rest upper bounds\n",
    "            idx = torch.arange(output_bounds.lower.size(0), device=device)\n",
    "            lower_true = output_bounds.lower[idx, labels]\n",
    "            upper_masked = output_bounds.upper.clone()\n",
    "            upper_masked[idx, labels] = float('-inf')\n",
    "            upper_masked_max, _ = upper_masked.max(dim=1)\n",
    "            robust_labels = (lower_true > upper_masked_max).sum().item()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            robust += robust_labels\n",
    "        print(f\"Eps: {eps}\")\n",
    "        print(f\"Total: {total}\")\n",
    "        print(f'Accuracy on images: {correct} ({100 * correct / total}%)')\n",
    "        print(f'Robustness accuracy on images: {robust} ({100 * robust / total}%)')\n",
    "\n",
    "test_model_robustness(bounded_model, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "255a7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eps: 0.01\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 546 (5.46%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.02\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 1 (0.01%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.03\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.04\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.05\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.060000000000000005\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.06999999999999999\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.08\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.09\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n",
      "Eps: 0.09999999999999999\n",
      "Total: 10000\n",
      "Accuracy on images: 9253 (92.53%)\n",
      "Robustness accuracy on images: 0 (0.0%)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for eps in np.arange(0.01, 0.11, 0.01):\n",
    "    test_model_robustness(bounded_model, eps)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68b8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
